This paper discusses the problem of using attention weights in transformer models to explain model decisions, specifically the unreliable nature of attention weights as explanations. The authors propose two methods, attention rollout and attention flow, for approximating attention to input tokens given attention weights. Both methods are based on modeling the information flow in the network with a directed acyclic graph. The authors show that these methods yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients, and provide better visualizations of how input tokens contribute to a predicted output compared to raw attention. The techniques proposed here serve as a diagnostic tool for visualization and debugging.
The article explores the usefulness of attention weights in transformer models for the task of predicting verb number agreement. The authors train a transformer model with GPT-2 blocks and visualize attention weights with figures 1 and 2. They observe that attention weights become more uniform in deeper layers, indicating the need to track down attention weights all the way back to the input layer. The authors then introduce two alternatives to raw attention weights, attention rollout and attention flow, and compare their usefulness with blank-out scores and input gradients. They find that attention rollout and attention flow are better correlated with blank-out scores and input gradients than raw attention weights, indicating their potential usefulness in analyzing transformer models.
The text discusses different methods to analyze attention in transformer models for verb prediction. Attention rollout and attention flow are introduced to track information propagation from input to higher layers, with attention flow treating the attention graph as a flow network. The text also notes the importance of considering residual connections and averaging attention over all heads in the analysis. Additionally, appendix a.1 explains how to analyze individual heads using attention roll-out and flow.
The article discusses two methods, attention rollout and attention flow, for analyzing attention in deep learning models. Both methods can be computed in polynomial time and provide additional information beyond raw attention. Attention rollout is more focused while attention flow is more generalized. The article includes analysis and discussion of the methods, as well as correlations with blank-out scores and input gradients.
The article compares attention flow and attention rollout methods for interpreting attention weights in models. Attention flow views weights as capacities, while attention rollout views them as proportion factors, making it stricter but less accurate. Both methods are applied to two BERT models and compared for accuracy in predicting sentiment analysis and resolving pronouns. While caution is advised in interpreting the weights, the methods are task and architecture agnostic and can be easily employed in any self-attention model. Future work may involve using gradient-based attribution methods or adjusting attention weights.
This is a list of research papers related to transformers and their interpretability, including topics such as attention visualization, data augmentation, and language understanding.
The first citation mentions a group of researchers who authored a paper on HuggingFace's Transformers, which is a technology for natural language processing. The second citation pertains to a conference paper on neural image caption generation with visual attention, authored by Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio.
The appendices include a section on analyzing attention weights in multi-head setups. It discusses the option of analyzing attention heads separately or averaging them to create a single attention graph. It cautions against assuming that there is no mixing of information between heads and suggests using attention rollout and attention flow to analyze the role of each head in isolation. To avoid the assumption of no mixing, the "input attention" is computed by treating all layers below the layer of interest as single head layers and summing the attentions of all heads. An example is given for computing attention rollout for a specific layer using the single head assumption.
