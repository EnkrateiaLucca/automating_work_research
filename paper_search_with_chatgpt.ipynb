{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Summarize Papers with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  530k  100  530k    0     0   339k      0  0:00:01  0:00:01 --:--:--  339k\n",
      "The paper presents a new framework called \"machines of finite depth\" that formally describes artificial neural networks and their architectures. Unlike neural networks, machines have a precise definition and several properties, including modularity, efficiency, and differentiability. The framework proves that the backward pass of a machine is also a machine and can be computed without overhead. The paper provides a unified implementation that generalizes several classical neural network architectures and their respective backpropagation rules. The authors also address the issue of defining deep neural networks and propose two solutions.\n",
      "The article discusses two different approaches to deep learning: one involves defining models through a simplified, domain-specific language for easier optimization, but can limit innovation; the other involves differentiable programming, where all code can be treated as a model, but has practical and theoretical drawbacks. The authors aim to establish a unified framework for deep learning, where deep neural networks can be defined in terms of a unique parametric machine, allowing for simplified architecture design and mathematical precision. The framework is language-agnostic and efficient for gradient computation. The contribution lies in unifying various neural network architectures and providing a theoretical framework for deep learning.\n",
      "The article discusses the concept of \"machines\" as a theoretical framework for building complex neural network architectures with shortcuts. The authors present the machine equation and its corresponding resolvent, which links to deep neural networks and backpropagation. They argue that these machines can be solved efficiently under certain conditions and can be combined under independence assumptions. Additionally, they introduce the concept of parametric machines and discuss how to differentiate their output with respect to the input and parameters. The authors implement these ideas in the Julia programming language and describe how they can be used as standalone algorithms or as layers within traditional neural network architectures.\n",
      "This article discusses the conditions for which the mapping idf is invertible and presents practical strategies for computing it and its derivative in relation to deep equilibrium models. The paper develops an approach to layer composition that utilizes a global space and endofunction to provide a mathematical framework for mapping. The article compares this new approach to traditional function composition and explains how it can be used to solve fixed-point problems efficiently. The paper outlines a streamlined approach that draws on functional analysis and is more efficient than previous versions of the framework. The article concludes by exploring the relationship between f and layer composition and clarifying how this relationship can be used to compute the derivative of the output with respect to the input and parameters.\n",
      "The text discusses a framework for recovering neural networks as a particular case of a machine learning model. The framework is based on the observation that the sum of all layers in a neural network and the input function alone are sufficient to determine the output function. The authors propose a definition of a machine as an endofunction with a unique input-output mapping. The text also mentions the possibility of using machines with higher-order differentiability for gradient-based hyperparameter optimization.\n",
      "This passage asserts that a function f is a machine if and only if the composed identity function and f is an isomorphism. The resolvent of f is then constructed using the inverse of the identity-f composition. Proposition 2 states that the derivative and dual of a machine are also machines, with resolvents provided for each. These ideas will be useful in optimizing parameters for machines.\n",
      "The text discusses proof and examples related to machines and their resolvents. Through differentiation, it shows that df(x0) is a machine with resolvent drf(x0), and that the dual of the resolvent is (id df(x0)')-1. The text then gives examples of standard sequential neural networks and bounded linear operators, where the resolvent can be explicitly constructed. If a bounded linear operator is nilpotent, then it is also a machine and its resolvent can be expressed as (id+f+f^2+...+f^(n-1))^(-1).\n",
      "This section discusses the overlap and distinction between sequential neural networks and nilpotent operators, and introduces the concept of machines of finite depth. Machines of finite depth are a general class of endofunctions whose resolvents can also be constructed explicitly. The definition of depth and a procedure to compute the resolvent of endofunctions of finite depth are provided, and the concept of co-filtration of a normed vector space is introduced. The depth of a function is defined as the length of its shortest depth co-filtration.\n",
      "The passage discusses depth co/filtrations for endofunctions, and how they can be combined to build depth co/filtrations for more complex functions. It also discusses a proposition related to depth co/filtrations for a given machine, and provides a proof. Additionally, it provides examples of endofunctions of finite depth, including sequential neural networks. The passage concludes by mentioning the development of tools in section 2.3 that will make proving these examples more straightforward.\n",
      "The article discusses the concept of a depth filtration and depth sequence in the context of solving machine equations. A depth sequence is a sequence of functions that progressively approximate solutions of the machine equation. The article also introduces the idea of machine independence, which allows for the creation of complex machines from simpler ones. This concept is important for composability, and it is used to decompose deep neural networks into layers. The article concludes with a theorem that shows that if a function admits a depth filtration, it is a machine.\n",
      "The article presents two different strategies for solving a linear machine with shortcuts on 5 nodes. The first approach is eï¬ƒcient, updating only the i-th node at each step. The second approach, where fhi+g is evaluated at each step, is shown to be less efficient as some connections need to be recomputed several times. An optimized sequence is proposed that defers the computation of each connection until the input value is fully determined. Figure 1 shows the connectivity graph and accumulated activation values for each node for each approach.\n",
      "Definition 4 defines whether a function f1 in a normed vector space depends on another function f2. If f1 does not change when f2 is added to a multiple of a point in the space, then f1 does not depend on f2. Otherwise, f1 depends on f2. This definition is useful in computing resolvents and can be used to show whether two functions respect each other. The definition can also be expressed as the differential of f1 factoring through a quotient. The sets of functions that satisfy this definition are vector spaces.\n",
      "Theorem 2 states that given machines f1 and f2 of depths d1 and d2 respectively, where f1 does not depend on f2, then the sum f1+f2 is also a machine of depth not greater than d1+d2, and the resolvent of f1+f2 is equal to rf2rf1. If f2 also does not depend on f1, then rf1+f2=rf1+rf2 and depth of f1+f2 is at most max(d1, d2). The proof uses composition of isomorphisms and the bounds on depth can be derived from depth filtrations.\n",
      "This is a description of a neural network with complex shortcuts. It maps inputs (x1, x2, x3, x4, ..., x8) to outputs (y1, y2, y3, y4, ..., y8) via five layers of functions (f1, f2, f3, f4, f5). The output values are computed by combining the input values and applying the functions in a specific order.\n",
      "The text discusses a theorem that establishes a link between independent machines and compositions of layers in feedforward neural networks. It states that if the building blocks have finite depth, then the sum also has finite depth. This allows for efficient computation of the machine's resolvent. The text also discusses how classical backpropagation-based optimization can be translated into this framework. The choice of endofunctions (architectures and weights) is restricted to smooth parameterized families of functions.\n",
      "The article discusses parametric machines and their parametric resolvent function. The resolvent function can be used to calculate partial derivatives with respect to the parameters and inputs. The structure and cost of the backward pass are comparable to those of the forward pass. The article provides a proof for the equations that hold for the partial derivatives of the resolvent function.\n",
      "The relevance of Theorem 3 is twofold. Firstly, it provides a practical approach to backpropagation for parametric machines. Secondly, it guarantees that the computational complexity of the backward pass is comparable to that of the forward pass in a broad class of practical cases. In section 3, the authors analyze several standard and non-standard architectures in the machine framework, provide a general implementation strategy, and discuss memory usage and performance for both the forward and backward pass. They use a dual machine to compute cotangent vectors, which allows for efficient backpropagation.\n",
      "The equation (9) provides a rule to backpropagate (v0,u0) to both input and parameter space. The computation of the dual machine has the same structure and computational complexity as the forward pass. The nonlinearity is pointwise and the derivative can be computed pointwise too. However, selecting a linear operator wp and a pointwise nonlinearity \u001b under the constraint that wp+\u001b is a machine of finite depth is a practical obstacle. The solution is to adopt a general strategy starting from classical existing layers and partitions on index spaces. The linear component of the machine is defined as a modified version of lp, and it is straightforward to verify that it satisfies the required conditions.\n",
      "The given text describes a depth co/filtration for wp+ which makes wp+ a machine of depth at most 2n+1. It then discusses a generalized multi-layer perceptron in which x[c], a tensor with one index, is adapted with the notation of the previous section. The machine equation and its backward pass are computed via the dual machine computation. Finally, the text explains the procedure for generalized multi-layer perceptrons and equivariant architectures, which include machines with underlying linear operator wpis translation-equivariant.\n",
      "The text describes a comparison of runtime for different models and devices, focusing on the ratio of runtime for the backward pass over the forward pass in dense, convolutional, and recurrent models with small and medium problem sizes. The results indicate that the runtimes of the backward and forward passes are comparable and backpropagation to parameter space requires an extra operation.\n",
      "The algorithm computes the forward and backward pass for a non-equivariant machine, using arrays for y, z, u, and v with specific equations for updating them. The input corresponds to a shift in the output for convolutional and recurrent neural networks. The equivariant case is similar, considering one-dimensional convolutions with a discrete grid and a tensor of three indices for the convolutional kernel. The partition is denoted by i0, ..., in and the update equations involve y, z, and l.\n",
      "The machine equation is a generalized version of convolutional and recurrent neural networks. The backward pass can be computed through a dual machine computation that uses transposed convolution. Different partitions of the equation lead to different architectures, such as deep convolutional networks with shortcuts or recurrent neural networks with shortcuts in depth and time. The memory usage of machines is different from traditional neural networks, as all the units at all depths are stored in memory and updated in place in a blockwise fashion. However, machines and neural networks have comparable memory usage during training because the values are needed to compute gradients by backpropagation and stored in memory by the automatic differentiation engine.\n",
      "The authors of the article propose a mathematical framework for studying deep neural networks. They define the notion of a machine, which generalizes the computation of a feedforward neural network and encompasses both equivariant and non-equivariant neural network architectures. They aim to strike a balance between domain-specific languages and differentiable programming by providing a principled notion of machine. They use a general procedure to define several classes of machines of finite depth, which involves an alternation between linear and nonlinear components. The composition of layers is replaced by the sum and sequentiality is replaced by the weaker notion of independence. They describe independence conditions to ensure that the sum of machines is again a machine, and provide a method for computing its resolvent.\n",
      "The article discusses a computational framework for defining machines with finite depth, which allows for efficient computation of both the forward and backward passes. The framework considers the dependency structure of a collection of machines to determine the order of composition, and the backward pass is framed as a resolvent computation. The article suggests that this approach may be useful in models where derivatives are used in the forward pass, such as gradient-based regularization or neural partial differential equations. The framework often generates architectures with shortcut connections, which can be regularized in various ways, such as setting parameters to zero, penalizing large differences between adjacent parameters, or choosing a representation of the parameter space with an associated notion of smoothness. The project was devised and developed by the authors, who also developed the software to implement the framework.\n",
      "This is a list of references from various publications related to machine learning and data analysis. The publications cover various topics such as deep equilibrium models, hyperparameter optimization, topology-geometrical theory, extensible programming, and benchmarking in noisy environments. The references also include papers on finding sparse neural networks, double backpropagation, compiling machine learning programs, and joint inference and input optimization in equilibrium networks.\n",
      "The given sources include articles and papers related to different fields such as neuroscience, machine learning, and medical imaging. These sources cover topics such as neuronal networks, differentiable programming, convolutional networks, hyperparameter optimization, automatic differentiation, and array programming.\n",
      "The provided sources cover various topics related to machine learning, including universal differential equations, deep learning platforms, support vector machines, automatic differentiation, and architecture search. The sources include journal articles, preprints, and software documentation.\n",
      "The first part of the summary lists two research papers on neural networks. The second part explains the concept of normed vector spaces and FrÃ©chet derivatives. It goes on to state a proposition detailing equivalent conditions for a continuously differentiable map to flow between quotients. The conditions involve closed subspaces and the behavior of derivatives. The proof shows that these conditions are equivalent to the map being able to lower to a quotient map.\n",
      "The passage discusses mathematical equations and their equivalences in relation to the concept of lowered maps. The author also provides information about numerical experiments conducted using Julia programming language for deep learning, with results reported for minimum times and the forward-to-backward time ratio.\n",
      "The table shows timings of forward and backward passes for different types of machines (dense, convolutional, and recurrent) with different sizes (small and medium) and on different devices (CPU and GPU). It also includes the backward over forward ratio. The benchmarks were done on a single minibatch for small (dimension 2) and medium (dimension 32) problem sizes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "import openai\n",
    "\n",
    "!curl -o paper.pdf https://arxiv.org/pdf/2204.12786.pdf\n",
    "\n",
    "# Set the string that will contain the summary     \n",
    "pdf_summary_text = \"\"\n",
    "# Open the PDF file\n",
    "pdf_file_path = \"paper.pdf\"\n",
    "# Read the PDF file using PyPDF2\n",
    "pdf_file = open(pdf_file_path, 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "# Loop through all the pages in the PDF file\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract the text from the page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text().lower()\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful research assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Summarize this: {page_text}\"},\n",
    "                            ],\n",
    "                                )\n",
    "    page_summary = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    pdf_summary_text+=page_summary + \"\\n\"\n",
    "    pdf_summary_file = pdf_file_path.replace(os.path.splitext(pdf_file_path)[1], \"_summary.txt\")\n",
    "    with open(pdf_summary_file, \"w+\") as file:\n",
    "        file.write(pdf_summary_text)\n",
    "\n",
    "pdf_file.close()\n",
    "\n",
    "with open(pdf_summary_file, \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Add saving to obsidian notes for access on the Ipad\n",
    "- [ ] Add the proper function to ask the pdf questions about the paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explore",
   "language": "python",
   "name": "explore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
